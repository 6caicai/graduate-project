#!/usr/bin/env python3
"""
åˆ†æé”™è¯¯åˆ†ç±»ï¼šåŒºåˆ†AIè¯¯åˆ¤ vs æ•°æ®é›†æ±¡æŸ“
åŸºäºç”¨æˆ·æä¾›çš„æ ·æœ¬ç»Ÿè®¡ï¼Œä¼°ç®—å„ç±»åˆ«çš„çœŸå®é”™è¯¯ç‡
"""
import json
from pathlib import Path
from collections import defaultdict, Counter
import re

REPO_ROOT = Path(__file__).resolve().parents[2]
BACKEND_DIR = REPO_ROOT / "campusphoto" / "backend"
ERROR_JSON = BACKEND_DIR / "error_cases.json"

# ç”¨æˆ·æä¾›çš„æ ·æœ¬ç»Ÿè®¡ï¼ˆAIè¯¯åˆ¤ vs æ•°æ®é›†é”™è¯¯ï¼‰
SAMPLE_STATS = {
    "åŠ¨ç‰©->è¡—æ™¯": {"ai_error": 3, "dataset_error": 0, "total": 3},
    "åŠ¨ç‰©->äººåƒ": {"ai_error": 188, "dataset_error": 26, "total": 214},
    "åŠ¨ç‰©->é£Ÿç‰©å’Œé™ç‰©": {"ai_error": 43, "dataset_error": 0, "total": 43},
    "åŠ¨ç‰©->è‡ªç„¶é£å…‰": {"ai_error": 730, "dataset_error": 0, "total": 730},
    "è¡—æ™¯->åŠ¨ç‰©": {"ai_error": 82, "dataset_error": 5, "total": 87},
    "äººåƒ->åŠ¨ç‰©": {"ai_error": 449, "dataset_error": 561, "total": 1010},
    "äººåƒ->è¡—æ™¯": {"ai_error": 48, "dataset_error": 50, "total": 98},
}

def load_error_cases():
    """åŠ è½½é”™è¯¯æ¡ˆä¾‹æ•°æ®"""
    if not ERROR_JSON.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ERROR_JSON}")
        return []
    
    with open(ERROR_JSON, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return data if isinstance(data, list) else []

def analyze_by_category():
    """æŒ‰ç±»åˆ«åˆ†æé”™è¯¯åˆ†å¸ƒ"""
    cases = load_error_cases()
    if not cases:
        return {}
    
    category_stats = defaultdict(list)
    
    for case in cases:
        if not isinstance(case, dict):
            continue
            
        expected = case.get("expected_theme", "")
        predicted = case.get("predicted_theme", "")
        file_path = case.get("file_path", "")
        
        if expected and predicted and expected != predicted:
            key = f"{expected}->{predicted}"
            category_stats[key].append({
                "file_path": file_path,
                "expected": expected,
                "predicted": predicted,
                "confidence": case.get("confidence", 0)
            })
    
    return dict(category_stats)

def calculate_error_rates():
    """åŸºäºæ ·æœ¬ç»Ÿè®¡è®¡ç®—å„ç±»åˆ«çš„é”™è¯¯ç‡"""
    category_stats = analyze_by_category()
    results = {}
    
    print("ğŸ“Š åŸºäºæ ·æœ¬ç»Ÿè®¡çš„é”™è¯¯åˆ†æ:")
    print("=" * 80)
    
    total_ai_errors = 0
    total_dataset_errors = 0
    total_cases = 0
    
    for category, cases in category_stats.items():
        case_count = len(cases)
        
        if category in SAMPLE_STATS:
            sample = SAMPLE_STATS[category]
            # è®¡ç®—é”™è¯¯ç‡
            ai_error_rate = sample["ai_error"] / sample["total"] if sample["total"] > 0 else 0
            dataset_error_rate = sample["dataset_error"] / sample["total"] if sample["total"] > 0 else 0
            
            # ä¼°ç®—å®é™…æ•°é‡
            estimated_ai_errors = int(case_count * ai_error_rate)
            estimated_dataset_errors = int(case_count * dataset_error_rate)
            
            results[category] = {
                "total_cases": case_count,
                "estimated_ai_errors": estimated_ai_errors,
                "estimated_dataset_errors": estimated_dataset_errors,
                "ai_error_rate": ai_error_rate,
                "dataset_error_rate": dataset_error_rate,
                "sample_size": sample["total"]
            }
            
            total_ai_errors += estimated_ai_errors
            total_dataset_errors += estimated_dataset_errors
            total_cases += case_count
            
            print(f"{category:20} | æ€»è®¡: {case_count:5d} | AIè¯¯åˆ¤: {estimated_ai_errors:4d} ({ai_error_rate:.1%}) | æ•°æ®é›†é”™è¯¯: {estimated_dataset_errors:4d} ({dataset_error_rate:.1%}) | æ ·æœ¬: {sample['total']:3d}")
        else:
            # æ²¡æœ‰æ ·æœ¬æ•°æ®çš„ç±»åˆ«ï¼ŒæŒ‰ä¿å®ˆä¼°è®¡ï¼ˆå‡è®¾50%æ˜¯AIé”™è¯¯ï¼‰
            estimated_ai_errors = int(case_count * 0.5)
            estimated_dataset_errors = case_count - estimated_ai_errors
            
            results[category] = {
                "total_cases": case_count,
                "estimated_ai_errors": estimated_ai_errors,
                "estimated_dataset_errors": estimated_dataset_errors,
                "ai_error_rate": 0.5,
                "dataset_error_rate": 0.5,
                "sample_size": 0
            }
            
            total_ai_errors += estimated_ai_errors
            total_dataset_errors += estimated_dataset_errors
            total_cases += case_count
            
            print(f"{category:20} | æ€»è®¡: {case_count:5d} | AIè¯¯åˆ¤: {estimated_ai_errors:4d} (50.0%) | æ•°æ®é›†é”™è¯¯: {estimated_dataset_errors:4d} (50.0%) | æ ·æœ¬: æ— ")
    
    print("=" * 80)
    print(f"{'æ€»è®¡':20} | æ€»è®¡: {total_cases:5d} | AIè¯¯åˆ¤: {total_ai_errors:4d} ({total_ai_errors/total_cases:.1%}) | æ•°æ®é›†é”™è¯¯: {total_dataset_errors:4d} ({total_dataset_errors/total_cases:.1%})")
    
    return results

def generate_summary_report():
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    results = calculate_error_rates()
    
    print("\nğŸ“‹ æ™ºèƒ½åˆ†ç±»é”™è¯¯åˆ†ææ€»ç»“:")
    print("=" * 60)
    
    # æŒ‰AIé”™è¯¯ç‡æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1]["ai_error_rate"], reverse=True)
    
    print("ğŸ”´ AIè¯¯åˆ¤æœ€ä¸¥é‡çš„ç±»åˆ«:")
    for category, stats in sorted_results[:5]:
        print(f"  {category}: {stats['estimated_ai_errors']} å¼  ({stats['ai_error_rate']:.1%})")
    
    print("\nğŸŸ¡ æ•°æ®é›†æ±¡æŸ“æœ€ä¸¥é‡çš„ç±»åˆ«:")
    sorted_by_dataset = sorted(results.items(), key=lambda x: x[1]["dataset_error_rate"], reverse=True)
    for category, stats in sorted_by_dataset[:5]:
        if stats["dataset_error_rate"] > 0:
            print(f"  {category}: {stats['estimated_dataset_errors']} å¼  ({stats['dataset_error_rate']:.1%})")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("  1. ä¼˜å…ˆæ¸…ç†æ•°æ®é›†æ±¡æŸ“ä¸¥é‡çš„ç±»åˆ«")
    print("  2. é’ˆå¯¹AIè¯¯åˆ¤ä¸¥é‡çš„ç±»åˆ«è¿›è¡Œæ¨¡å‹ä¼˜åŒ–")
    print("  3. è€ƒè™‘é‡æ–°æ ‡æ³¨æˆ–è¿‡æ»¤ä½è´¨é‡æ•°æ®")

if __name__ == "__main__":
    generate_summary_report()
