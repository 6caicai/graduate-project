import os
import sys
import json
import subprocess
from pathlib import Path
import unicodedata
import re
from typing import List, Dict, Any, Tuple


REPO_ROOT = Path(__file__).resolve().parents[2]
BACKEND_DIR = REPO_ROOT / "campusphoto" / "backend"
ERROR_JSON = BACKEND_DIR / "error_cases.json"
EXP_DIR = REPO_ROOT / "campusphoto" / "ai" / "exp"
# ç»“æœè¾“å‡ºç›®å½•æ”¹ä¸º ai/exp/result
RESULT_DIR = EXP_DIR / "result"

# YOLO æ ‡æ³¨æ”¯æŒ
try:
    from utils.yolo_image_classifier import yolo_classifier  # type: ignore
except Exception:
    yolo_classifier = None  # å»¶è¿Ÿå¤±è´¥å¤„ç†


def open_image(image_path: Path) -> bool:
    """ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¨‹åºæ‰“å¼€å›¾ç‰‡ï¼ˆmacOS/Linux/Windows å…¼å®¹ï¼‰ã€‚"""
    try:
        if sys.platform == "darwin":
            subprocess.run(["open", str(image_path)], check=False)
        elif os.name == "nt":
            os.startfile(str(image_path))  # type: ignore[attr-defined]
        else:
            subprocess.run(["xdg-open", str(image_path)], check=False)
        return True
    except Exception:
        return False


def load_error_cases_raw() -> List[Dict[str, Any]]:
    """è¯»å– error_cases.json çš„åŸå§‹è®°å½•åˆ—è¡¨ã€‚"""
    if not ERROR_JSON.exists():
        return []

    try:
        data = json.loads(ERROR_JSON.read_text(encoding="utf-8"))
        if isinstance(data, list):
            return [x for x in data if isinstance(x, dict)]
        return []
    except Exception:
        return []


def normalize_path(p: Any) -> Path:
    """Normalize path strings to avoid false negatives on existence checks.

    - Strip surrounding whitespace
    - Remove zero-width and control characters
    - Unicode normalize to NFC (macOS paths may be NFD)
    - Normalize path separators
    """
    if not isinstance(p, str):
        return Path(p)
    s = p.strip()
    # Remove zero-width and control chars
    s = re.sub(r"[\u200B-\u200D\uFEFF\u202A-\u202E\u2060\u2066-\u2069\u0000-\u001F]", "", s)
    # Unicode normalize
    s = unicodedata.normalize('NFC', s)
    # Normalize separators
    s = os.path.normpath(s)
    return Path(s)


def to_nfd(s: str) -> str:
    try:
        return unicodedata.normalize('NFD', s)
    except Exception:
        return s


def path_exists_any(p: Path) -> bool:
    """Check existence trying both NFC and NFD normalized strings."""
    s = str(p)
    # try current
    if p.exists() or os.path.lexists(s):
        return True
    # try NFD
    s_nfd = to_nfd(s)
    if os.path.exists(s_nfd) or os.path.lexists(s_nfd):
        return True
    # try NFC (redundant but explicit)
    s_nfc = unicodedata.normalize('NFC', s)
    if os.path.exists(s_nfc) or os.path.lexists(s_nfc):
        return True
    return False


def debug_path_info(p: Path) -> None:
    s = str(p)
    s_nfc = unicodedata.normalize('NFC', s)
    s_nfd = to_nfd(s)
    print("\nğŸ” è°ƒè¯•è·¯å¾„ä¿¡æ¯:")
    print(f" raw: {s}")
    print(f" len: {len(s)}")
    print(f" NFC exists: {os.path.exists(s_nfc)}  lexists: {os.path.lexists(s_nfc)}")
    print(f" NFD exists: {os.path.exists(s_nfd)}  lexists: {os.path.lexists(s_nfd)}")
    # æ‰“å°æœ«å°¾æ–‡ä»¶åå„å­—ç¬¦çš„ code pointï¼ˆæœ€å¤šå‰åå„20å­—ç¬¦ï¼‰
    tail = os.path.basename(s)
    hexs = ' '.join(hex(ord(ch)) for ch in tail)
    print(f" basename: {tail}")
    print(f" codepoints: {hexs}")
    # åˆ—ä¸€ä¸‹çˆ¶ç›®å½•ï¼Œå¸®åŠ©å¯¹æ¯”
    parent = os.path.dirname(s)
    try:
        names = os.listdir(parent)
        print(f" parent: {parent}")
        print(f" has {len(names)} entries, sample: {names[:10]}")
    except Exception as e:
        print(f" listdir error: {e}")


def build_filename_index(root: Path) -> Dict[str, List[Path]]:
    """Index all files under root by basename (with NFC and NFD normalized keys)."""
    index: Dict[str, List[Path]] = {}
    if not root.exists():
        return index
    for dirpath, _, filenames in os.walk(root):
        for name in filenames:
            if not name:
                continue
            name_nfc = unicodedata.normalize('NFC', name)
            name_nfd = unicodedata.normalize('NFD', name)
            p = Path(dirpath) / name
            index.setdefault(name, []).append(p)
            index.setdefault(name_nfc, []).append(p)
            index.setdefault(name_nfd, []).append(p)
    return index


def try_repair_missing_paths(missing: List[Path]) -> Dict[Path, Path]:
    """Attempt to repair missing paths by matching basenames under EXP_DIR.

    Preference:
      1) candidates in same top-level class folder (derived from original path after 'exp')
      2) otherwise unique basename match anywhere under EXP_DIR
    Returns a mapping from old_missing_path -> new_found_path
    """
    repaired: Dict[Path, Path] = {}
    if not EXP_DIR.exists():
        return repaired
    index = build_filename_index(EXP_DIR)
    for mp in missing:
        name = os.path.basename(str(mp))
        name_nfc = unicodedata.normalize('NFC', name)
        name_nfd = unicodedata.normalize('NFD', name)
        cands = index.get(name, []) + index.get(name_nfc, []) + index.get(name_nfd, [])
        if not cands:
            continue
        # derive class folder from original path
        parts = Path(str(mp)).parts
        cls = None
        for i, seg in enumerate(parts):
            if seg == 'exp' and i + 1 < len(parts):
                cls = parts[i + 1]
                break
        if cls:
            same_cls = [c for c in cands if ('/exp/' + cls + '/') in str(c)]
            if len(same_cls) == 1:
                repaired[mp] = same_cls[0]
                continue
            if len(same_cls) > 1:
                # if multiple in same class, pick the shortest full path as heuristic
                repaired[mp] = sorted(same_cls, key=lambda p: len(str(p)))[0]
                continue
        # fallback: unique basename across all
        unique = list({str(c): c for c in cands}.values())
        if len(unique) == 1:
            repaired[mp] = unique[0]
        else:
            # pick shortest path as heuristic
            repaired[mp] = sorted(unique, key=lambda p: len(str(p)))[0]
    return repaired

def extract_existing_paths(cases: List[Dict[str, Any]]) -> List[Path]:
    paths: List[Path] = []
    for item in cases:
        p = item.get("file_path") or item.get("path")
        if isinstance(p, str):
            path_obj = normalize_path(p)
            if path_exists_any(path_obj):
                paths.append(path_obj)
    return paths


def extract_missing_paths(cases: List[Dict[str, Any]]) -> List[Path]:
    paths: List[Path] = []
    for item in cases:
        p = item.get("file_path") or item.get("path")
        if isinstance(p, str):
            path_obj = normalize_path(p)
            if not path_exists_any(path_obj):
                paths.append(path_obj)
    return paths


def build_groups(cases: List[Dict[str, Any]], mode: str = "gt_pred") -> Dict[str, List[Path]]:
    """æŒ‰ gt->pred åˆ†ç»„ï¼Œå€¼ä¸ºå­˜åœ¨çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨ã€‚
    
    å…¼å®¹å¤šç§å­—æ®µå‘½åï¼ˆä¸­è‹±æ–‡ï¼‰ï¼Œå¹¶åœ¨ç¼ºå¤± gt æ—¶å°½é‡ä»è·¯å¾„ä¸­æ¨æ–­ï¼ˆai/exp/åä¸€çº§ç›®å½•ï¼‰ã€‚
    """
    # ä¼˜å…ˆä» JSON è¯»å– GT/Pred å­—æ®µï¼›è‹¥ GT ç¼ºå¤±åˆ™å›é€€ä»è·¯å¾„æ¨å¯¼
    default_gt_keys = [
        "expected_theme", "gt", "truth", "label", "ground_truth", "true_label", "expected",
        "expected_label", "çœŸå®", "çœŸå®ç±»åˆ«", "åº”ä¸º", "æ ‡ç­¾", "çœŸå®åˆ†ç±»", "groundtruth"
    ]
    default_pred_keys = [
        "predicted_theme", "pred", "prediction", "predicted", "pred_label", "model_pred", "yolo_pred",
        "detected", "detected_label", "ai", "ai_label", "ai_theme",
        "é¢„æµ‹", "é¢„æµ‹ç±»åˆ«", "è¯¯åˆ†ä¸º", "è¢«è¯¯åˆ†ä¸º", "é¢„æµ‹åˆ†ç±»"
    ]

    def first_str(item: Dict[str, Any], keys: List[str]) -> Any:
        for k in keys:
            v = item.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
        return None

    def derive_gt_from_path(p: str) -> Any:
        try:
            parts = Path(p).parts
            # å¯»æ‰¾ "exp"ï¼Œå–å…¶åä¸€çº§ç›®å½•ä½œä¸º gtï¼ˆè‹¥å­˜åœ¨ï¼‰
            for i, seg in enumerate(parts):
                if seg == "exp":
                    if i + 1 < len(parts):
                        return parts[i + 1]
                    break
        except Exception:
            pass
        return None

    groups: Dict[str, List[Path]] = {}
    for item in cases:
        p = item.get("file_path") or item.get("path")
        if not isinstance(p, str):
            continue
        path_obj = normalize_path(p)

        # è¿è¡Œæ—¶é€‰å®šçš„é”®ä¼˜å…ˆï¼ˆå¯ç”±å¤–éƒ¨è®¾ç½®ï¼‰ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤é”®
        gt_keys = getattr(build_groups, "_gt_keys", None) or default_gt_keys
        pred_keys = getattr(build_groups, "_pred_keys", None) or default_pred_keys

        # å…ˆä» JSON è¯»å– GT/Predï¼›è‹¥ GT ç¼ºå¤±åˆ™å›é€€è·¯å¾„
        gt = first_str(item, gt_keys)
        pred = first_str(item, pred_keys)
        if not gt:
            gt = derive_gt_from_path(str(path_obj))

        if mode == "gt_only":
            if not isinstance(gt, str):
                continue
            key = f"{gt}"
        else:
            if not isinstance(gt, str) or not isinstance(pred, str):
                continue
            key = f"{gt}->{pred}"
        groups.setdefault(key, []).append(path_obj)
    return groups


def detect_candidate_keys(sample: List[Dict[str, Any]], max_samples: int = 1000) -> Tuple[List[str], List[str]]:
    """ä»æ ·æœ¬ä¸­ç»Ÿè®¡å¯èƒ½çš„å­—ç¬¦ä¸²é”®ï¼Œè¿”å›å€™é€‰ gt é”®ã€pred é”®åˆ—è¡¨ï¼ˆæŒ‰å‡ºç°é¢‘æ¬¡æ’åºï¼‰ã€‚"""
    from collections import Counter

    gt_aliases = set([
        "gt", "truth", "label", "ground_truth", "true_label", "expected",
        "expected_label", "actual", "çœŸå®", "çœŸå®ç±»åˆ«", "åº”ä¸º", "æ ‡ç­¾", "çœŸå®åˆ†ç±»", "groundtruth"
    ])
    pred_aliases = set([
        "pred", "prediction", "predicted", "pred_label", "model_pred", "yolo_pred",
        "detected", "detected_label", "ai", "ai_label", "ai_theme",
        "é¢„æµ‹", "é¢„æµ‹ç±»åˆ«", "è¯¯åˆ†ä¸º", "è¢«è¯¯åˆ†ä¸º", "é¢„æµ‹åˆ†ç±»"
    ])

    gt_counter: Counter[str] = Counter()
    pred_counter: Counter[str] = Counter()

    for i, item in enumerate(sample):
        if i >= max_samples:
            break
        if not isinstance(item, dict):
            continue
        for k, v in item.items():
            if isinstance(v, str) and v.strip():
                if k in gt_aliases:
                    gt_counter[k] += 1
                if k in pred_aliases:
                    pred_counter[k] += 1

    return [k for k, _ in gt_counter.most_common()], [k for k, _ in pred_counter.most_common()]



def deduplicate_keep_order(paths: List[Path]) -> List[Path]:
    seen: Dict[str, bool] = {}
    result: List[Path] = []
    for p in paths:
        key = str(p.resolve())
        if key not in seen:
            seen[key] = True
            result.append(p)
    return result


def open_batch(paths: List[Path]) -> None:
    """ä¸€æ¬¡æ€§æ‰“å¼€ä¸€æ‰¹å›¾ç‰‡ã€‚macOS ä½¿ç”¨å•æ¬¡ open å¤šæ–‡ä»¶ï¼Œå…¶å®ƒå¹³å°é€ä¸ªæ‰“å¼€ã€‚"""
    if not paths:
        return
    try:
        if sys.platform == "darwin":
            subprocess.run(["open", *[str(p) for p in paths]], check=False)
        elif os.name == "nt":
            for p in paths:
                os.startfile(str(p))  # type: ignore[attr-defined]
        else:
            for p in paths:
                subprocess.run(["xdg-open", str(p)], check=False)
    except Exception:
        pass


def sanitize_group_dir_name(group_key: str) -> str:
    name = group_key.strip().replace("/", "-")
    # å»é™¤ä¸å®‰å…¨å­—ç¬¦
    return re.sub(r"[^\w\-\u4e00-\u9fa5><= ]+", "_", name)


def annotate_batch(paths: List[Path], group_key: str, base_dir: Path = RESULT_DIR) -> int:
    """ä½¿ç”¨ YOLO å¯¹ä¸€æ‰¹å›¾ç‰‡ç”Ÿæˆæ ‡æ³¨å›¾ï¼Œä¿å­˜åˆ° result/<group_key>/ ä¸‹ã€‚
    è¿”å›æˆåŠŸç”Ÿæˆçš„æ•°é‡ã€‚
    """
    if not paths:
        return 0
    base_dir.mkdir(parents=True, exist_ok=True)
    group_dir = base_dir / sanitize_group_dir_name(group_key)
    group_dir.mkdir(parents=True, exist_ok=True)

    if yolo_classifier is None:
        print("âŒ YOLO åˆ†ç±»å™¨æœªå¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ ‡æ³¨å›¾")
        return 0

    success = 0
    for p in paths:
        try:
            # ä¸ºé¿å…é‡åå†²çªï¼ŒåŠ  annotated_ å‰ç¼€
            outfile = group_dir / ("annotated_" + Path(str(p)).name)
            ok = yolo_classifier.create_annotated_image(str(p), str(outfile))
            if ok:
                success += 1
            else:
                # è‹¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¤åˆ¶åŸå›¾ä»¥å ä½ï¼ˆå¯é€‰ï¼‰
                pass
        except Exception as e:
            print(f"   âš ï¸ æ ‡æ³¨å¤±è´¥: {p} - {e}")
    print(f"âœ… æœ¬æ‰¹æ ‡æ³¨å®Œæˆï¼Œå…±ç”Ÿæˆ {success}/{len(paths)} å¼ ï¼Œè¾“å‡ºç›®å½•: {group_dir}")
    return success


def annotate_all_groups(groups: Dict[str, List[Path]], dest_root: Path = RESULT_DIR) -> None:
    if yolo_classifier is None:
        print("âŒ YOLO åˆ†ç±»å™¨æœªå¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ ‡æ³¨å›¾")
        return
    total_groups = len(groups)
    print(f"ğŸ› ï¸ å¼€å§‹ä¸ºæ‰€æœ‰åˆ†ç»„ç”Ÿæˆæ ‡æ³¨å›¾ï¼Œå…± {total_groups} ç»„...")
    done_groups = 0
    total_images = sum(len(v) for v in groups.values())
    done_images = 0
    for gk, gpaths in groups.items():
        print(f"\nğŸ“‚ åˆ†ç»„: {gk}  å›¾ç‰‡æ•°: {len(gpaths)}")
        cnt = annotate_batch(gpaths, gk, base_dir=dest_root)
        done_groups += 1
        done_images += cnt
        print(f"è¿›åº¦: ç»„ {done_groups}/{total_groups}  ç´¯è®¡æˆåŠŸ {done_images}/{total_images}")
    print(f"\nâœ… å…¨éƒ¨åˆ†ç»„æ ‡æ³¨å®Œæˆã€‚è¾“å‡ºæ ¹ç›®å½•: {dest_root}")


def main() -> None:
    cases = load_error_cases_raw()
    if not cases:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„è¯¯åˆ†ç±»è®°å½•ï¼ˆè¯·æä¾› campusphoto/backend/error_cases.jsonï¼‰")
        sys.exit(1)

    print(f"âœ… å·²åŠ è½½ {len(cases)} æ¡è¯¯åˆ†ç±»è®°å½•ï¼ˆå°†ç›´æ¥å°è¯•æ‰“å¼€è·¯å¾„ï¼Œæ‰“å¼€å¤±è´¥å³è§†ä¸ºä¸å­˜åœ¨ï¼‰")

    # é»˜è®¤ç›´æ¥ä½¿ç”¨ expected_theme ä¸ predicted_theme è¿›è¡Œ gt->pred åˆ†ç»„
    setattr(build_groups, "_gt_keys", ["expected_theme"])  # è‹¥æ— åˆ™å›é€€è·¯å¾„
    setattr(build_groups, "_pred_keys", ["predicted_theme"])  # è‹¥æ— å†ç”¨é»˜è®¤å€™é€‰
    mode = "gt_pred"

    # æ„å»ºåˆ†ç»„ï¼Œå¹¶æŒ‰ç»„å†…å›¾ç‰‡æ•°é‡é™åºæ˜¾ç¤º
    groups = build_groups(cases, mode=mode)
    if not groups:
        print("âŒ æœªèƒ½æ„å»ºåˆ†ç»„ï¼ˆè¯·ç¡®è®¤ JSON ä¸­åŒ…å«å¯ç”¨çš„è·¯å¾„ï¼Œæˆ–æä¾›é¢„æµ‹å­—æ®µï¼‰")
        sys.exit(1)

    sorted_groups = sorted(groups.items(), key=lambda kv: len(kv[1]), reverse=True)

    # ä¸å†é¢„å…ˆæ£€æµ‹ç¼ºå¤±ï¼Œç›´æ¥æŒ‰åˆ†ç»„æ‰“å¼€ï¼Œå¦‚æ‰“å¼€å¤±è´¥åˆ™ç”±ç³»ç»ŸæŠ¥é”™

    while True:
        print("\nå¯ç”¨åˆ†ç»„ï¼š")
        for idx, (k, v) in enumerate(sorted_groups, 1):
            print(f"  {idx}. {k}  (å…± {len(v)} å¼ )")
        print("é€‰æ‹©åºå·è¿›å…¥è¯¥åˆ†ç»„ï¼›è¾“å…¥ q é€€å‡ºï¼›è¾“å…¥ r é‡æ–°åŠ è½½åˆ†ç»„ï¼›è¾“å…¥ x ä¸€é”®å¯¼å‡ºæ‰€æœ‰åˆ†ç»„")

        try:
            choice = input("> ").strip().lower()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ å·²é€€å‡º")
            break

        if choice == "q":
            print("ğŸ‘‹ å·²é€€å‡º")
            break
        if choice == "r":
            cases = load_error_cases_raw()
            groups = build_groups(cases)
            sorted_groups = sorted(groups.items(), key=lambda kv: len(kv[1]), reverse=True)
            continue
        if choice == "x":
            annotate_all_groups(dict(sorted_groups), dest_root=RESULT_DIR)
            continue

        if not choice.isdigit():
            print("â„¹ï¸ è¯·è¾“å…¥åˆ†ç»„åºå·æˆ– q é€€å‡º")
            continue

        idx = int(choice)
        if idx < 1 or idx > len(sorted_groups):
            print("â„¹ï¸ åºå·è¶…å‡ºèŒƒå›´")
            continue

        group_key, group_paths = sorted_groups[idx - 1]
        group_paths = deduplicate_keep_order(group_paths)
        batch_size = 100
        total = len(group_paths)
        start = 0
        print(f"\nğŸ“‚ åˆ†ç»„: {group_key}  å…± {total} å¼ ")
        while start < total:
            end = min(start + batch_size, total)
            print(f"\nğŸ“¦ å³å°†æ‰“å¼€ç¬¬ {start+1} - {end} å¼ ï¼ˆå…± {total}ï¼‰")
            print("æ“ä½œ: å›è½¦=æ‰“å¼€æœ¬æ‰¹, y=æ ‡æ³¨å¹¶å¯¼å‡ºæœ¬æ‰¹, a=æ ‡æ³¨å¹¶å¯¼å‡ºæ•´ç»„, n=è·³è¿‡æœ¬æ‰¹, b=è¿”å›, q=é€€å‡º")
            try:
                sub = input("> ").strip().lower()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å·²é€€å‡º")
                return

            if sub == "q":
                print("ğŸ‘‹ å·²é€€å‡º")
                return
            if sub == "b":
                break
            if sub == "n":
                start = end
                continue
            if sub == "y":
                annotate_batch(group_paths[start:end], group_key)
                start = end
                continue
            if sub == "a":
                print("ğŸ› ï¸ æ­£åœ¨ä¸ºæ•´ç»„ç”Ÿæˆæ ‡æ³¨å›¾ï¼Œè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´...")
                annotate_batch(group_paths, group_key)
                # ä»…ç”Ÿæˆï¼Œä¸æ”¹å˜å½“å‰åˆ†é¡µä½ç½®
                continue

            open_batch(group_paths[start:end])
            start = end


if __name__ == "__main__":
    main()


