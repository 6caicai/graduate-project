#!/usr/bin/env python3
"""
å…¨é¢åˆ†æé”™è¯¯åˆ†ç±»ï¼šå…ˆç»Ÿè®¡æ‰€æœ‰ç±»åˆ«ï¼Œå†åŸºäºæ ·æœ¬ä¼°ç®—AIè¯¯åˆ¤æ•°é‡
"""
import json
from pathlib import Path
from collections import defaultdict, Counter

REPO_ROOT = Path(__file__).resolve().parents[2]
BACKEND_DIR = REPO_ROOT / "campusphoto" / "backend"
ERROR_JSON = BACKEND_DIR / "error_cases.json"

# ç”¨æˆ·æä¾›çš„æ ·æœ¬ç»Ÿè®¡ï¼ˆAIè¯¯åˆ¤ vs æ•°æ®é›†é”™è¯¯ï¼‰
SAMPLE_STATS = {
    "åŠ¨ç‰©->è¡—æ™¯": {"ai_error": 3, "dataset_error": 0, "total": 3},
    "åŠ¨ç‰©->äººåƒ": {"ai_error": 188, "dataset_error": 26, "total": 214},
    "åŠ¨ç‰©->é£Ÿç‰©å’Œé™ç‰©": {"ai_error": 43, "dataset_error": 0, "total": 43},
    "åŠ¨ç‰©->è‡ªç„¶é£å…‰": {"ai_error": 730, "dataset_error": 0, "total": 730},
    "è¡—æ™¯->åŠ¨ç‰©": {"ai_error": 82, "dataset_error": 5, "total": 87},
    "äººåƒ->åŠ¨ç‰©": {"ai_error": 449, "dataset_error": 561, "total": 1010},
    "äººåƒ->è¡—æ™¯": {"ai_error": 48, "dataset_error": 50, "total": 98},
}

def load_error_cases():
    """åŠ è½½é”™è¯¯æ¡ˆä¾‹æ•°æ®"""
    if not ERROR_JSON.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ERROR_JSON}")
        return []
    
    with open(ERROR_JSON, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return data if isinstance(data, list) else []

def analyze_all_categories():
    """åˆ†ææ‰€æœ‰é”™è¯¯ç±»åˆ«"""
    cases = load_error_cases()
    if not cases:
        return {}
    
    category_stats = defaultdict(list)
    theme_counts = Counter()
    
    for case in cases:
        if not isinstance(case, dict):
            continue
            
        expected = case.get("expected_theme", "")
        predicted = case.get("predicted_theme", "")
        
        if expected and predicted:
            theme_counts[expected] += 1
            if expected != predicted:
                key = f"{expected}->{predicted}"
                category_stats[key].append(case)
    
    return dict(category_stats), dict(theme_counts)

def estimate_ai_errors():
    """åŸºäºæ ·æœ¬ç»Ÿè®¡ä¼°ç®—AIè¯¯åˆ¤æ•°é‡"""
    category_stats, theme_counts = analyze_all_categories()
    
    print("ğŸ“Š é”™è¯¯åˆ†ç±»å®Œæ•´ç»Ÿè®¡:")
    print("=" * 100)
    print(f"{'ç±»åˆ«':25} | {'æ€»æ•°':6} | {'AIè¯¯åˆ¤':8} | {'æ•°æ®é›†é”™è¯¯':10} | {'AIè¯¯åˆ¤ç‡':8} | {'æ ·æœ¬æ•°':6}")
    print("-" * 100)
    
    total_cases = 0
    total_ai_errors = 0
    total_dataset_errors = 0
    
    # æŒ‰æ€»æ•°æ’åºæ˜¾ç¤º
    sorted_categories = sorted(category_stats.items(), key=lambda x: len(x[1]), reverse=True)
    
    for category, cases in sorted_categories:
        case_count = len(cases)
        total_cases += case_count
        
        if category in SAMPLE_STATS:
            sample = SAMPLE_STATS[category]
            ai_error_rate = sample["ai_error"] / sample["total"] if sample["total"] > 0 else 0
            dataset_error_rate = sample["dataset_error"] / sample["total"] if sample["total"] > 0 else 0
            
            estimated_ai_errors = int(case_count * ai_error_rate)
            estimated_dataset_errors = int(case_count * dataset_error_rate)
            
            total_ai_errors += estimated_ai_errors
            total_dataset_errors += estimated_dataset_errors
            
            print(f"{category:25} | {case_count:6} | {estimated_ai_errors:8} | {estimated_dataset_errors:10} | {ai_error_rate:7.1%} | {sample['total']:6}")
        else:
            # æ²¡æœ‰æ ·æœ¬æ•°æ®çš„ç±»åˆ«ï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
            estimated_ai_errors = int(case_count * 0.5)
            estimated_dataset_errors = case_count - estimated_ai_errors
            
            total_ai_errors += estimated_ai_errors
            total_dataset_errors += estimated_dataset_errors
            
            print(f"{category:25} | {case_count:6} | {estimated_ai_errors:8} | {estimated_dataset_errors:10} | {'50.0%':8} | {'æ— ':6}")
    
    print("-" * 100)
    print(f"{'æ€»è®¡':25} | {total_cases:6} | {total_ai_errors:8} | {total_dataset_errors:10} | {total_ai_errors/total_cases:7.1%} | {'':6}")
    
    return category_stats

def show_theme_distribution():
    """æ˜¾ç¤ºå„ä¸»é¢˜çš„åˆ†å¸ƒæƒ…å†µ"""
    cases = load_error_cases()
    theme_counts = Counter()
    
    for case in cases:
        if isinstance(case, dict):
            expected = case.get("expected_theme", "")
            if expected:
                theme_counts[expected] += 1
    
    print("\nğŸ“ˆ å„ä¸»é¢˜åˆ†å¸ƒ:")
    print("=" * 40)
    for theme, count in theme_counts.most_common():
        print(f"{theme:15} | {count:6} å¼ ")
    
    print(f"{'æ€»è®¡':15} | {sum(theme_counts.values()):6} å¼ ")

def generate_detailed_report():
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
    print("ğŸ” é”™è¯¯åˆ†ç±»è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("=" * 120)
    
    # 1. æ˜¾ç¤ºä¸»é¢˜åˆ†å¸ƒ
    show_theme_distribution()
    
    # 2. åˆ†ææ‰€æœ‰é”™è¯¯ç±»åˆ«
    category_stats = estimate_ai_errors()
    
    # 3. é‡ç‚¹åˆ†ææœ‰æ ·æœ¬æ•°æ®çš„ç±»åˆ«
    print("\nğŸ¯ åŸºäºæ ·æœ¬ç»Ÿè®¡çš„é‡ç‚¹åˆ†æ:")
    print("=" * 80)
    
    for category, sample in SAMPLE_STATS.items():
        if category in category_stats:
            total = len(category_stats[category])
            ai_rate = sample["ai_error"] / sample["total"]
            dataset_rate = sample["dataset_error"] / sample["total"]
            
            estimated_ai = int(total * ai_rate)
            estimated_dataset = int(total * dataset_rate)
            
            print(f"{category}:")
            print(f"  æ ·æœ¬: {sample['total']} å¼  (AIè¯¯åˆ¤: {sample['ai_error']}, æ•°æ®é›†é”™è¯¯: {sample['dataset_error']})")
            print(f"  å…¨é‡: {total} å¼  (ä¼°ç®—AIè¯¯åˆ¤: {estimated_ai}, ä¼°ç®—æ•°æ®é›†é”™è¯¯: {estimated_dataset})")
            print(f"  é”™è¯¯ç‡: AI {ai_rate:.1%}, æ•°æ®é›† {dataset_rate:.1%}")
            print()

if __name__ == "__main__":
    generate_detailed_report()
